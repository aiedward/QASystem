{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNQANet():\n",
    "    def __init__(self, pretrained_embedding, encoder_units_number=[300, 100], attention_size=[100], hidden_rnn_size=[100], learning_rate=0.001, log_dir='./logs', model_path='./RNNQANet'):\n",
    "        tf.reset_default_graph()\n",
    "        self.question = tf.placeholder(shape=[None, None], dtype=tf.int32, name='question')\n",
    "        self.context = tf.placeholder(shape=[None, None], dtype=tf.int32, name='context')\n",
    "        self.y_start = tf.placeholder(shape=[None], dtype=tf.int32, name='y_start')\n",
    "        self.y_end = tf.placeholder(shape=[None], dtype=tf.int32, name='y_end')\n",
    "        self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='dropout_keep_prob')\n",
    "        self.global_step = 0\n",
    "        self.model_path=model_path\n",
    "        with tf.variable_scope('embedding', initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            W = tf.Variable(pretrained_embedding, trainable=True, dtype=tf.float32, name='W_emb')\n",
    "            self.question_input = tf.nn.embedding_lookup(ids=self.question, params=W)\n",
    "            self.context_input = tf.nn.embedding_lookup(ids=self.context, params=W)\n",
    "        \n",
    "        with tf.variable_scope('context_encoder', initializer=tf.contrib.layers.xavier_initializer(uniform=True)) as scope:\n",
    "            # Using Bidirectional RNN to encode context\n",
    "            # u_c=BiRNN(e_c)\n",
    "            fcell, bcell = self._biGRUs(encoder_units_number, activation=tf.nn.relu, keep_prob=self.dropout_keep_prob)\n",
    "            self.context_output, self.c_state = tf.nn.bidirectional_dynamic_rnn(inputs=self.context_input, cell_fw=fcell, cell_bw=bcell, dtype=tf.float32, scope=scope)\n",
    "            self.context_output = tf.concat(self.context_output, axis=-1)\n",
    "            self.context_output = tf.contrib.layers.layer_norm(self.context_output)\n",
    "            tf.summary.histogram('context_encoder', self.context_output)\n",
    "        \n",
    "        with tf.variable_scope('question_encoder', initializer=tf.contrib.layers.xavier_initializer(uniform=True)) as scope:\n",
    "            # Using Bidirectional RNN to encode question\n",
    "            # u_q=BiRNN(e_q)\n",
    "            fcell, bcell = self._biGRUs(encoder_units_number, activation=tf.nn.relu, keep_prob=self.dropout_keep_prob)\n",
    "            self.question_output, self.q_state = tf.nn.bidirectional_dynamic_rnn(inputs=self.question_input, cell_fw=fcell, cell_bw=bcell, dtype=tf.float32, scope=scope)\n",
    "            self.question_output = tf.concat(self.question_output, axis=-1)\n",
    "            self.question_output = tf.contrib.layers.layer_norm(self.question_output)\n",
    "            tf.summary.histogram('question_encoder', self.question_output)\n",
    "        \n",
    "        with tf.variable_scope('co_attention', initializer=tf.contrib.layers.xavier_initializer(uniform=True)):\n",
    "            # Co-attention: context -> question\n",
    "            # a_cq=attn_biRNN(u_c,u_q)\n",
    "            self.cq_att = self.gated_attention(self.context_output, self.question_output, hidden=attention_size, scope='cq_attention')\n",
    "            cqfcell, cqbcell = self._biGRUs(hidden_rnn_size, activation=tf.nn.relu, keep_prob=self.dropout_keep_prob)\n",
    "            self.cq_att, _ = tf.nn.bidirectional_dynamic_rnn(inputs=self.cq_att, cell_fw=cqfcell, cell_bw=cqbcell, dtype=tf.float32, scope=tf.get_variable_scope().name + '/cq_attention_rnn')\n",
    "            self.cq_att = tf.concat(self.cq_att, axis=-1)\n",
    "            self.cq_att = tf.contrib.layers.layer_norm(self.cq_att)\n",
    "            tf.summary.histogram('cq_att', self.cq_att)\n",
    "            \n",
    "            # Co-attention: question -> context\n",
    "            # a_qc=attn_biRNN(u_q,u_c)\n",
    "            self.qc_att = self.gated_attention(self.question_output, self.context_output, hidden=attention_size, scope='qc_attention')\n",
    "            qcfcell, qcbcell = self._biGRUs(hidden_rnn_size, activation=tf.nn.relu, keep_prob=self.dropout_keep_prob)\n",
    "            self.qc_att, _ = tf.nn.bidirectional_dynamic_rnn(inputs=self.qc_att, cell_fw=qcfcell, cell_bw=qcbcell, dtype=tf.float32, scope=tf.get_variable_scope().name + '/qc_attention_rnn')\n",
    "            self.qc_att = tf.concat(self.qc_att, axis=-1)\n",
    "            self.qc_att = tf.contrib.layers.layer_norm(self.qc_att)\n",
    "            tf.summary.histogram('qc_att', self.qc_att)\n",
    "        \n",
    "        with tf.variable_scope('self_attention', initializer=tf.contrib.layers.xavier_initializer(uniform=True)):\n",
    "            # Self-attention: a_cq -> a_cq\n",
    "            # a_cc=attn_biRNN(a_cq,a_cq)\n",
    "            self.cc_att = self.gated_attention(self.cq_att, self.cq_att, hidden=attention_size, scope='cc_attention')\n",
    "            ccfcell, ccbcell = self._biGRUs(hidden_rnn_size, activation=tf.nn.relu, keep_prob=self.dropout_keep_prob)\n",
    "            self.cc_att, _ = tf.nn.bidirectional_dynamic_rnn(inputs=self.cc_att, cell_fw=ccfcell, cell_bw=ccbcell, dtype=tf.float32, scope=tf.get_variable_scope().name + '/cc_attention_rnn')\n",
    "            self.cc_att = tf.concat(self.cc_att, axis=-1)\n",
    "            self.cc_att = tf.contrib.layers.layer_norm(self.cc_att)\n",
    "            tf.summary.histogram('cc_att', self.cc_att)\n",
    "            \n",
    "            # Self-attention: a_qc -> a_qc\n",
    "            # a_qq=attn_biRNN(a_qc,a_qc)\n",
    "            self.qq_att = self.gated_attention(self.qc_att, self.qc_att, hidden=attention_size, scope='qq_attention')\n",
    "            qqfcell, qqbcell = self._biGRUs(hidden_rnn_size, activation=tf.nn.relu, keep_prob=self.dropout_keep_prob)\n",
    "            self.qq_att, _ = tf.nn.bidirectional_dynamic_rnn(inputs=self.qq_att, cell_fw=qqfcell, cell_bw=qqbcell, dtype=tf.float32, scope=tf.get_variable_scope().name + '/qq_attention_rnn')\n",
    "            self.qq_att = tf.concat(self.qq_att, axis=-1)\n",
    "            self.qq_att = tf.contrib.layers.layer_norm(self.qq_att)\n",
    "            tf.summary.histogram('qq_att', self.qq_att)\n",
    "        \n",
    "        with tf.variable_scope('output_layer', initializer=tf.contrib.layers.xavier_initializer(uniform=True)) as scope:\n",
    "            # Output-attention: a_cq -> a_qc\n",
    "            # a_o1,a_o2=attn_biRNN(a_cc,a_qq)\n",
    "            output_att = self.gated_attention(self.cc_att, self.qq_att, hidden=attention_size, scope='output_attention')\n",
    "            output_att = tf.concat([self.context_output, output_att], axis=-1)\n",
    "            fcell, bcell = self._biGRUs(hidden_rnn_size, activation=tf.nn.relu, keep_prob=self.dropout_keep_prob)\n",
    "            output, _ = tf.nn.bidirectional_dynamic_rnn(inputs=output_att, cell_fw=fcell, cell_bw=bcell, dtype=tf.float32, scope=scope)\n",
    "            \n",
    "            # use forward output to generate y1\n",
    "            self.start_output = output[0]\n",
    "            self.start_output = tf.contrib.layers.layer_norm(self.start_output)\n",
    "            tf.summary.histogram('start_output', self.start_output)\n",
    "            \n",
    "            # use backward output to generate y2\n",
    "            self.end_output = output[1]\n",
    "            self.end_output = tf.contrib.layers.layer_norm(self.end_output)\n",
    "            tf.summary.histogram('end_output', self.end_output)\n",
    "        \n",
    "        with tf.variable_scope('start_decoder', initializer=tf.contrib.layers.xavier_initializer(uniform=True)):\n",
    "            # p_y1=RNN(a_o1)\n",
    "            cell = [self._add_GRU(50, activation=tf.nn.relu), self._add_GRU(25, activation=tf.nn.relu), self._add_GRU(1, activation=tf.nn.relu)]\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells=cell, state_is_tuple=True)\n",
    "            self.y_predict_start, _ = tf.nn.dynamic_rnn(cell=cell, inputs=self.start_output, dtype=tf.float32)\n",
    "            self.y_predict_start = tf.unstack(self.y_predict_start, axis=-1)[0]\n",
    "            tf.summary.histogram('y_predict_start', self.y_predict_start)\n",
    "            self.y_predict_start_softmax = tf.nn.softmax(self.y_predict_start)\n",
    "            self.y_predict_start_index = tf.argmax(self.y_predict_start_softmax, axis=1)\n",
    "            self.y_start_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.y_predict_start, labels=self.y_start)\n",
    "            tf.summary.scalar('start_loss', tf.reduce_mean(self.y_start_loss))\n",
    "        \n",
    "        with tf.variable_scope('end_decoder', initializer=tf.contrib.layers.xavier_initializer(uniform=True)):\n",
    "            # p_y2=RNN(a_o2)\n",
    "            cell = [self._add_GRU(50, activation=tf.nn.relu), self._add_GRU(25, activation=tf.nn.relu), self._add_GRU(1, activation=tf.nn.relu)]\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells=cell, state_is_tuple=True)\n",
    "            self.y_predict_end, _ = tf.nn.dynamic_rnn(cell=cell, inputs=self.end_output, dtype=tf.float32)\n",
    "            self.y_predict_end = tf.unstack(self.y_predict_end, axis=-1)[0]\n",
    "            tf.summary.histogram('y_predict_end', self.y_predict_end)\n",
    "            self.y_predict_end_softmax = tf.nn.softmax(self.y_predict_end)\n",
    "            self.y_predict_end_index = tf.argmax(self.y_predict_end_softmax, axis=1)\n",
    "            self.y_end_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.y_predict_end, labels=self.y_end)\n",
    "            tf.summary.scalar('end_loss', tf.reduce_mean(self.y_end_loss))\n",
    "        \n",
    "        with tf.variable_scope('train'):\n",
    "            self.optimizier = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.total_loss = tf.reduce_mean((self.y_start_loss + self.y_end_loss) / 2.0)\n",
    "            tf.summary.scalar('total_loss', self.total_loss)\n",
    "            self.train_op = self.optimizier.minimize(self.total_loss)\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.merge_op = tf.summary.merge_all()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(self.init_op)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.writer = tf.summary.FileWriter(log_dir,graph=self.session.graph)\n",
    "    \n",
    "    def _add_dense_layer(self, inputs, output_shape, drop_keep_prob, act=tf.nn.tanh, use_bias=True):\n",
    "        output = inputs\n",
    "        for n in output_shape:\n",
    "            output = tf.layers.dense(output, n, activation=act, use_bias=use_bias)\n",
    "            output = tf.nn.dropout(output, drop_keep_prob)\n",
    "        return output\n",
    "    \n",
    "    def gated_attention(self, inputs, memory, hidden, keep_prob=1.0, is_train=None, scope=\"dot_attention\", self_attention=False):\n",
    "        with tf.variable_scope(scope):\n",
    "            with tf.variable_scope(\"attention\"):\n",
    "                # u=W1*u_i\n",
    "                inputs_ = self._add_dense_layer(inputs, hidden, keep_prob, act=tf.nn.relu, use_bias=False)\n",
    "                \n",
    "                # v=W2*v_i\n",
    "                memory_ = self._add_dense_layer(memory, hidden, keep_prob, act=tf.nn.relu, use_bias=False)\n",
    "                \n",
    "                # s=softmax(u*v)\n",
    "                outputs = tf.matmul(inputs_, tf.transpose(memory_, [0, 2, 1]))\n",
    "                logits = tf.nn.softmax(outputs)\n",
    "                \n",
    "                # l=s*v_i\n",
    "                outputs = tf.matmul(logits, memory)\n",
    "                \n",
    "                # r=[u_i,l]\n",
    "                result = tf.concat([inputs, outputs], axis=-1)\n",
    "            with tf.variable_scope(\"gate\"):\n",
    "                # g=\\sigma(W_g*r)\n",
    "                gate = self._add_dense_layer(result, [result.shape[-1]], keep_prob, act=tf.nn.sigmoid, use_bias=False)\n",
    "                # o=g*r\n",
    "                return result * gate\n",
    "    \n",
    "    def _biGRUs(self, units_number, activation=tf.nn.relu, keep_prob=1.0):\n",
    "        fcell = [self._add_GRU(units_number=n, keep_prob=keep_prob, activation=activation) for n in units_number]\n",
    "        fcell = tf.contrib.rnn.MultiRNNCell(cells=fcell, state_is_tuple=True)\n",
    "        bcell = [self._add_GRU(units_number=n, keep_prob=keep_prob, activation=activation) for n in units_number]\n",
    "        bcell = tf.contrib.rnn.MultiRNNCell(cells=bcell, state_is_tuple=True)\n",
    "        return fcell, bcell\n",
    "    \n",
    "    def _add_GRU(self, units_number, activation=tf.nn.tanh, keep_prob=1.0):\n",
    "        cell = tf.contrib.rnn.GRUCell(units_number, activation=activation)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "        return cell\n",
    "    \n",
    "    def build_feed_dict(self, context, question, y_start, y_end, drop_keep_prob=0.7):\n",
    "        feed_dict = {\n",
    "            self.question: question,\n",
    "            self.context: context,\n",
    "            self.y_start: y_start,\n",
    "            self.y_end: y_end,\n",
    "            self.dropout_keep_prob: drop_keep_prob\n",
    "        }\n",
    "        return feed_dict\n",
    "    \n",
    "    def train(self, context, question, y1, y2, drop_keep_prob=0.85, record_interval=10):\n",
    "        feed_dict = {\n",
    "            self.question: question,\n",
    "            self.context: context,\n",
    "            self.y_start: y1,\n",
    "            self.y_end: y2,\n",
    "            self.dropout_keep_prob: drop_keep_prob\n",
    "        }\n",
    "        if self.global_step % record_interval == 0:\n",
    "            _, loss, summaries = self.session.run([self.train_op, self.total_loss, self.merge_op], feed_dict=feed_dict)\n",
    "            self.writer.add_summary(summaries, self.global_step)\n",
    "        else:\n",
    "            _, loss = self.session.run([self.train_op, self.total_loss], feed_dict=feed_dict)\n",
    "        self.global_step += 1\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, context, question, y1, y2, drop_keep_prob=1.0):\n",
    "        feed_dict = {\n",
    "            self.question: question,\n",
    "            self.context: context,\n",
    "            self.y_start: y1,\n",
    "            self.y_end: y2,\n",
    "            self.dropout_keep_prob: drop_keep_prob\n",
    "        }\n",
    "        loss = self.session.run([self.total_loss], feed_dict=feed_dict)\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = loss[0]\n",
    "        summary_value.tag = 'evaluate_loss'\n",
    "        self.writer.add_summary(summary, self.global_step)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, context, question):\n",
    "        feed_dict = {\n",
    "            self.question: question,\n",
    "            self.context: context,\n",
    "            self.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        start, end = self.session.run([self.y_predict_start_index, self.y_predict_end_index], feed_dict=feed_dict)\n",
    "        return start, end\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.saver.restore(self.session, self.model_path + '/rnnqanet')\n",
    "    \n",
    "    def save_model(self, ):\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.mkdir(self.model_path)\n",
    "        model_file = self.model_path + '/rnnqanet'\n",
    "        self.saver.save(self.session, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training corpus context&question length 431 60\n",
      "dev corpus context&question length 156 33\n",
      "test corpus context&question length 155 36\n"
     ]
    }
   ],
   "source": [
    "training_corpus=np.load('train_corpus.npy')\n",
    "test_corpus=np.load('test_corpus.npy')\n",
    "dev_corpus=np.load('dev_corpus.npy')\n",
    "weight_matrix=np.load('weight_matrix.npy')\n",
    "with open('word_index.pkl','rb') as f:\n",
    "    word_index=pickle.load(f)\n",
    "with open('inverted_word_index.pkl','rb') as f:\n",
    "    inverted_word_index=pickle.load(f)\n",
    "with open('train_length.pkl','rb') as f:\n",
    "    max_train_context_length,max_train_question_length=pickle.load(f)\n",
    "with open('test_length.pkl','rb') as f:\n",
    "    max_test_context_length,max_test_question_length=pickle.load(f)\n",
    "with open('dev_length.pkl','rb') as f:\n",
    "    max_dev_context_length,max_dev_question_length=pickle.load(f)\n",
    "train_context_col, train_question_col = (max_train_context_length, max_train_context_length + max_train_question_length)\n",
    "dev_context_col, dev_question_col = (max_dev_context_length, max_dev_context_length + max_dev_question_length)\n",
    "test_context_col, test_question_col = (max_test_context_length, max_test_context_length + max_test_question_length)\n",
    "print('training corpus context&question length', max_train_context_length, max_train_question_length)\n",
    "print('dev corpus context&question length', max_dev_context_length, max_dev_question_length)\n",
    "print('test corpus context&question length', max_test_context_length, max_test_question_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNTrain(model_params, max_epoch=4, max_batch_size=128, checkpoint_interval=10, evaluate_interval=10, evaluate_batch_size=128):\n",
    "    rnn = RNNQANet(pretrained_embedding=weight_matrix,\n",
    "                   encoder_units_number=model_params['encoder_units_number'],\n",
    "                   attention_size=model_params['attention_size'],\n",
    "                   hidden_rnn_size=model_params['hidden_rnn_size'],\n",
    "                   learning_rate=model_params['learning_rate'],\n",
    "                   log_dir=model_params['log_dir'],\n",
    "                   model_path=model_params['model_path']\n",
    "                   )\n",
    "    global_step = 0\n",
    "    previous_save_loss = np.inf\n",
    "    for e in range(max_epoch):\n",
    "        np.random.shuffle(training_corpus)\n",
    "        epoch_loss = []\n",
    "        it = 0\n",
    "        while it < training_corpus.shape[0]:\n",
    "            max_context_length = max(np.sum(training_corpus[it:it + max_batch_size, :train_context_col] > 0, axis=1))\n",
    "            b_context = training_corpus[it:it + max_batch_size, :max_context_length]\n",
    "            max_question_length = max(np.sum(training_corpus[it:it + max_batch_size, train_context_col:train_question_col] > 0, axis=1))\n",
    "            b_question = training_corpus[it:it + max_batch_size, train_context_col:train_context_col + max_question_length]\n",
    "            b_y1 = training_corpus[it:it + max_batch_size, -2]\n",
    "            b_y2 = training_corpus[it:it + max_batch_size, -1]\n",
    "            loss = rnn.train(context=b_context, question=b_question, y1=b_y1, y2=b_y2, record_interval=1)\n",
    "            epoch_loss.append(loss)\n",
    "            print('epoch', e, 'step', global_step, 'iteration', it, 'iteration loss', loss, 'epoch mean loss', np.mean(epoch_loss))\n",
    "            it += max_batch_size\n",
    "            global_step += 1\n",
    "            if global_step % evaluate_interval == 0:\n",
    "                np.random.shuffle(dev_corpus)\n",
    "                batch_dev_context_length = max(np.sum(dev_corpus[:evaluate_batch_size, :dev_context_col] > 0, axis=1))\n",
    "                d_context = dev_corpus[:evaluate_batch_size, :batch_dev_context_length]\n",
    "                batch_dev_question_length = max(np.sum(dev_corpus[:evaluate_batch_size, dev_context_col:dev_question_col] > 0, axis=1))\n",
    "                d_question = dev_corpus[:evaluate_batch_size, dev_context_col:dev_context_col + batch_dev_question_length]\n",
    "                d_y1 = dev_corpus[:evaluate_batch_size, -2]\n",
    "                d_y2 = dev_corpus[:evaluate_batch_size, -1]\n",
    "                dev_loss = rnn.evaluate(d_context, d_question, d_y1, d_y2)\n",
    "                if global_step % checkpoint_interval == 0 and previous_save_loss > dev_loss[0]:\n",
    "                    rnn.save_model()\n",
    "                    previous_save_loss = dev_loss[0]\n",
    "                    print(global_step, 'save model @ val loss', dev_loss[0])\n",
    "                print(global_step, 'evaluate loss', dev_loss[0])\n",
    "    # make sure it is not overfiting\n",
    "    rnn.load_model()\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model_params = {\n",
    "    'encoder_units_number': [300, 100],\n",
    "    'attention_size': [100],\n",
    "    'hidden_rnn_size': [100],\n",
    "    'learning_rate': 0.001,\n",
    "    'log_dir': './logs/RNNModel',\n",
    "    'model_path': './RNNQANet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, max_batch_size=128):\n",
    "    predict_context = test_corpus[:, :test_context_col]\n",
    "    predict_question = test_corpus[:, test_context_col:test_question_col]\n",
    "    predict_result = []\n",
    "    predict_result.append(('id','answer'))\n",
    "    it = 0\n",
    "    qi = 0\n",
    "    while it < len(predict_question):\n",
    "        batch_max_context_length = max(np.sum(predict_context[it:it + max_batch_size, :max_test_context_length] > 0, axis=1))\n",
    "        b_context = predict_context[it:it + max_batch_size, :batch_max_context_length]\n",
    "        batch_max_question_length = max(np.sum(predict_question[it:it + max_batch_size, :max_test_question_length] > 0, axis=1))\n",
    "        b_question = predict_question[it:it + max_batch_size, :batch_max_question_length]\n",
    "        batch_result = np.array(model.predict(context=b_context, question=b_question)).T\n",
    "        for a, c in zip(batch_result, b_context):\n",
    "            y1 = min(a)\n",
    "            y2 = max(a)\n",
    "            result_token_index = c[y1:y2 + 1]\n",
    "            result_tokens = ' '.join(lmap(lambda x: inverted_word_index[x], result_token_index))\n",
    "            predict_result.append((qi, result_tokens))\n",
    "            qi += 1\n",
    "        print(it / len(predict_question))\n",
    "        it += max_batch_size\n",
    "    return predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RNNTrain(model_params=RNN_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=predict(model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}