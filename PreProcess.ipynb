{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.summarization.bm25 import BM25\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.json', 'r+') as f:\n",
    "    documents = json.loads(f.read())\n",
    "with open('training.json', 'r+') as f:\n",
    "    train = json.loads(f.read())\n",
    "with open('testing.json', 'r+') as f:\n",
    "    test = json.loads(f.read())\n",
    "with open('devel.json', 'r+') as f:\n",
    "    dev = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmap = lambda func, it: list(map(func, it))\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "n = lambda x: nlp(x, disable=['tagger', 'ner', 'textcat', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_answer_index(answer_tokens, doc_tokens):\n",
    "    doc_length = len(doc_tokens)\n",
    "    answer_length = len(answer_tokens)\n",
    "    for i in range(doc_length):\n",
    "        if doc_length - i < answer_length:\n",
    "            return (0, 0)\n",
    "        found = True\n",
    "        for j in range(answer_length):\n",
    "            found = answer_tokens[j].lower() == doc_tokens[i + j].lower()\n",
    "            if not found: break\n",
    "        if found:\n",
    "            return (i, i + answer_length - 1)\n",
    "    return (0, 0)\n",
    "def crop_pad(max_leng, word_index):\n",
    "    if len(word_index) > max_leng:\n",
    "        return word_index[:max_leng]\n",
    "    pad_leng = max_leng - len(word_index)\n",
    "    word_index = word_index + [0] * pad_leng\n",
    "    assert len(word_index) == max_leng\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return lmap(lambda x: x.text, n(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(documents, train, dev, test):\n",
    "    training_corpus = []\n",
    "    dev_corpus = []\n",
    "    weight_matrix = []\n",
    "    word_index = collections.defaultdict()\n",
    "    test_corpus = []\n",
    "    word_index.setdefault('', len(word_index))\n",
    "    weight_matrix.append(np.zeros(300, dtype=np.float32))\n",
    "    for d in tqdm(documents):\n",
    "        pure_paras = d['text']\n",
    "        pure_paras_text = lmap(lambda x: x.replace('\"', '').replace('`', '').replace('``', '').replace(\"''\", '').replace('``', ''), pure_paras)\n",
    "        docid = d['docid']\n",
    "        paras_tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=None, lowercase=True, min_df=0)\n",
    "        paras_score = paras_tfidf.fit_transform(pure_paras_text).toarray()\n",
    "        doc_para_bm25 = BM25(lmap(lambda x: lmap(lambda y: y.text, n(x)), pure_paras))\n",
    "        doc_para_bm25_average_idf = sum(map(lambda k: float(doc_para_bm25.idf[k]), doc_para_bm25.idf.keys())) / len(doc_para_bm25.idf.keys())\n",
    "        for pid, p in enumerate(pure_paras_text):\n",
    "            para_train_questions = list(filter(lambda x: x['docid'] == docid and x['answer_paragraph'] == pid, train))\n",
    "            para_dev_questions = list(filter(lambda x: x['docid'] == docid and x['answer_paragraph'] == pid, dev))\n",
    "            para_text = p.replace('\"', '').replace('`', '').replace('``', '').replace(\"''\", '').replace('``', '')\n",
    "            para = n(para_text)\n",
    "            p_sentences = list(para.sents)\n",
    "            para_tokens = list(para)\n",
    "            for t in para_tokens:\n",
    "                t_text = t.text\n",
    "                if t.is_oov:\n",
    "                    t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                if t_text not in word_index:\n",
    "                    word_index.setdefault(t_text, len(word_index))\n",
    "                    weight_matrix.append(t.vector)\n",
    "            for qid, q in enumerate(para_train_questions):\n",
    "                question_text = q['question'].replace('\"', '').replace('`', '').replace('``', '').replace(\"''\", '').replace('``', '')\n",
    "                question = n(question_text)\n",
    "                question_tokens = list(question)\n",
    "                question_token_index = []\n",
    "                for t in question_tokens:\n",
    "                    t_text = t.text\n",
    "                    if t.is_oov:\n",
    "                        t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                    if t.text not in word_index:\n",
    "                        word_index.setdefault(t_text, len(word_index))\n",
    "                        weight_matrix.append(t.vector)\n",
    "                    question_token_index.append(word_index[t_text])\n",
    "                answer_text = q['text'].replace('\"', '').replace('`', '').replace('``', '').replace(\"''\", '').replace('``', '')\n",
    "                answer = n(answer_text)\n",
    "                answer_tokens = list(answer)\n",
    "                answer_token_index = []\n",
    "                for t in answer_tokens:\n",
    "                    t_text = t.text\n",
    "                    if t.is_oov:\n",
    "                        t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                    if t.text not in word_index:\n",
    "                        word_index.setdefault(t_text, len(word_index))\n",
    "                        weight_matrix.append(t.vector)\n",
    "                    answer_token_index.append(word_index[t_text])\n",
    "                answer_token_text = lmap(lambda x: x.text, answer_tokens)\n",
    "                for s in p_sentences:\n",
    "                    target_sent = s\n",
    "                    sent_token_text = lmap(lambda x: x.text, target_sent)\n",
    "                    y1, y2 = lookup_answer_index(answer_tokens=answer_token_text, doc_tokens=sent_token_text)\n",
    "                    sent_token_index = []\n",
    "                    for t in target_sent:\n",
    "                        t_text = t.text\n",
    "                        if t.is_oov:\n",
    "                            t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                        if t.text not in word_index:\n",
    "                            word_index.setdefault(t_text, len(word_index))\n",
    "                            weight_matrix.append(t.vector)\n",
    "                        sent_token_index.append(word_index[t_text])\n",
    "                    if lmap(lambda x: x.lower(), answer_token_text) == lmap(lambda x: x.lower(), sent_token_text)[y1:y2 + 1]:\n",
    "                        training_corpus.append((sent_token_index, question_token_index, y1, y2))\n",
    "                        break\n",
    "            for qid, q in enumerate(para_dev_questions):\n",
    "                question_text = q['question'].replace('\"', '').replace('`', '').replace('``', '').replace(\"''\", '').replace('``', '')\n",
    "                question = n(question_text)\n",
    "                question_tokens = list(question)\n",
    "                question_token_index = []\n",
    "                for t in question_tokens:\n",
    "                    t_text = t.text\n",
    "                    if t.is_oov:\n",
    "                        t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                    if t.text not in word_index:\n",
    "                        word_index.setdefault(t_text, len(word_index))\n",
    "                        weight_matrix.append(t.vector)\n",
    "                    question_token_index.append(word_index[t_text])\n",
    "                answer_text = q['text'].replace('\"', '').replace('`', '').replace('``', '').replace(\"''\", '').replace('``', '')\n",
    "                answer = n(answer_text)\n",
    "                answer_tokens = list(answer)\n",
    "                answer_token_index = []\n",
    "                for t in answer_tokens:\n",
    "                    t_text = t.text\n",
    "                    if t.is_oov:\n",
    "                        t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                    if t.text not in word_index:\n",
    "                        word_index.setdefault(t_text, len(word_index))\n",
    "                        weight_matrix.append(t.vector)\n",
    "                    answer_token_index.append(word_index[t_text])\n",
    "                answer_token_text = lmap(lambda x: x.text, answer_tokens)\n",
    "                for s in p_sentences:\n",
    "                    target_sent = s\n",
    "                    sent_token_text = lmap(lambda x: x.text, target_sent)\n",
    "                    y1, y2 = lookup_answer_index(answer_tokens=answer_token_text, doc_tokens=sent_token_text)\n",
    "                    sent_token_index = []\n",
    "                    for t in target_sent:\n",
    "                        t_text = t.text\n",
    "                        if t.is_oov:\n",
    "                            t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                        if t.text not in word_index:\n",
    "                            word_index.setdefault(t_text, len(word_index))\n",
    "                            weight_matrix.append(t.vector)\n",
    "                        sent_token_index.append(word_index[t_text])\n",
    "                    if lmap(lambda x: x.lower(), answer_token_text) == lmap(lambda x: x.lower(), sent_token_text)[y1:y2 + 1]:\n",
    "                        dev_corpus.append((sent_token_index, question_token_index, y1, y2))\n",
    "                        break\n",
    "        test_questions = list(filter(lambda x: x['docid'] == docid, test))\n",
    "        for t in test_questions:\n",
    "            tid = t['id']\n",
    "            question_text = t['question'].replace('\"', '').replace('`', '').replace('``', '').replace(\"''\", '').replace('``', '')\n",
    "            question = n(question_text)\n",
    "            question_tokens = list(question)\n",
    "            question_token_index = []\n",
    "            for t in question_tokens:\n",
    "                t_text = t.text\n",
    "                if t.is_oov:\n",
    "                    t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                if t.text not in word_index:\n",
    "                    word_index.setdefault(t_text, len(word_index))\n",
    "                    weight_matrix.append(t.vector)\n",
    "                question_token_index.append(word_index[t_text])\n",
    "            q_para_tfidf_score = paras_tfidf.transform([question_text]).toarray()\n",
    "            para_tfidf_sim = np.dot(paras_score, q_para_tfidf_score.T).flatten()\n",
    "            para_tfidf_sim = (para_tfidf_sim - np.mean(para_tfidf_sim)) / (np.std(para_tfidf_sim) + 1e-10)\n",
    "\n",
    "            para_bm25_sim = np.array(doc_para_bm25.get_scores(lmap(lambda x: x.text, n(question_text)), doc_para_bm25_average_idf))\n",
    "            para_bm25_sim = (para_bm25_sim - np.mean(para_bm25_sim)) / (np.std(para_bm25_sim) + 1e-10)\n",
    "\n",
    "            para_total_sim = para_bm25_sim * 0.5 + para_tfidf_sim * 0.5\n",
    "\n",
    "            target_para_index = np.argmax(para_total_sim)\n",
    "            target_para = pure_paras_text[target_para_index]\n",
    "            target_para_sents = list(n(target_para).sents)\n",
    "            target_para_sents_text = lmap(lambda x: x.text, target_para_sents)\n",
    "\n",
    "            target_para_sents_tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=None, lowercase=True, min_df=0)\n",
    "            target_para_sents_tfidf_score = target_para_sents_tfidf.fit_transform(target_para_sents_text).toarray()\n",
    "            q_sent_score = target_para_sents_tfidf.transform([question_text]).toarray()\n",
    "            sent_tfidf_sim = np.dot(target_para_sents_tfidf_score, q_sent_score.T).flatten()\n",
    "\n",
    "            embed_sim = np.array(lmap(lambda x: x.similarity(question), target_para_sents))\n",
    "\n",
    "            sent_bm25 = BM25(lmap(lambda x: lmap(lambda y: y.text, list(x)), list(n(target_para).sents)))\n",
    "            sent_bm25_average_idf = sum(map(lambda k: float(sent_bm25.idf[k]), sent_bm25.idf.keys())) / len(sent_bm25.idf.keys())\n",
    "            sent_bm25_sim = np.array(sent_bm25.get_scores(lmap(lambda x: x.text, question_tokens), sent_bm25_average_idf))\n",
    "\n",
    "            sent_tfidf_sim = (sent_tfidf_sim - np.mean(sent_tfidf_sim)) / (np.std(sent_tfidf_sim) + 1e-10)\n",
    "            embed_sim = (embed_sim - np.mean(embed_sim)) / (np.std(embed_sim) + 1e-10)\n",
    "            sent_bm25_sim = (sent_bm25_sim - np.mean(sent_bm25_sim)) / (np.std(sent_bm25_sim) + 1e-10)\n",
    "            total_sim = (sent_tfidf_sim + embed_sim + sent_bm25_sim) / 3\n",
    "            sent_index = np.argmax(total_sim)\n",
    "            target_sent = target_para_sents[sent_index]\n",
    "            sent_token_index = []\n",
    "            for t in target_sent:\n",
    "                t_text = t.text\n",
    "                if t.is_oov:\n",
    "                    t.vocab.set_vector(t_text, np.random.uniform(-1, 1, (300,)))\n",
    "                if t.text not in word_index:\n",
    "                    word_index.setdefault(t_text, len(word_index))\n",
    "                    weight_matrix.append(t.vector)\n",
    "                sent_token_index.append(word_index[t_text])\n",
    "            test_corpus.append((tid, sent_token_index, question_token_index))\n",
    "\n",
    "    weight_matrix = np.array(weight_matrix)\n",
    "    invert_word_index = {v: k for k, v in word_index.items()}\n",
    "\n",
    "    train_context = lmap(lambda x: x[0], training_corpus)\n",
    "    train_questions = lmap(lambda x: x[1], training_corpus)\n",
    "    train_y1 = lmap(lambda x: x[2], training_corpus)\n",
    "    train_y2 = lmap(lambda x: x[3], training_corpus)\n",
    "    dev_context = lmap(lambda x: x[0], dev_corpus)\n",
    "    dev_questions = lmap(lambda x: x[1], dev_corpus)\n",
    "    dev_y1 = lmap(lambda x: x[2], dev_corpus)\n",
    "    dev_y2 = lmap(lambda x: x[3], dev_corpus)\n",
    "    test_context = lmap(lambda x: x[1], test_corpus)\n",
    "    test_questions = lmap(lambda x: x[2], test_corpus)\n",
    "\n",
    "    max_train_context_length = max(lmap(lambda x: len(x), train_context))\n",
    "    max_train_question_length = max(lmap(lambda x: len(x), train_questions))\n",
    "    max_dev_context_length = max(lmap(lambda x: len(x), dev_context))\n",
    "    max_dev_question_length = max(lmap(lambda x: len(x), dev_questions))\n",
    "    max_test_context_length = max(lmap(lambda x: len(x), test_context))\n",
    "    max_test_question_length = max(lmap(lambda x: len(x), test_questions))\n",
    "\n",
    "    train_context = np.array(lmap(lambda x: crop_pad(max_train_context_length, x), train_context))\n",
    "    train_questions = np.array(lmap(lambda x: crop_pad(max_train_question_length, x), train_questions))\n",
    "    dev_context = np.array(lmap(lambda x: crop_pad(max_dev_context_length, x), dev_context))\n",
    "    dev_questions = np.array(lmap(lambda x: crop_pad(max_dev_question_length, x), dev_questions))\n",
    "    test_context = np.array(lmap(lambda x: crop_pad(max_test_context_length, x), test_context))\n",
    "    test_questions = np.array(lmap(lambda x: crop_pad(max_test_question_length, x), test_questions))\n",
    "    train_y = np.array([train_y1, train_y2]).T\n",
    "    dev_y = np.array([dev_y1, dev_y2]).T\n",
    "\n",
    "    concatenated_training_corpus = np.concatenate((train_context, train_questions, train_y), axis=-1)\n",
    "    concatenated_dev_corpus = np.concatenate((dev_context, dev_questions, dev_y), axis=-1)\n",
    "    concatenated_test_corpus = np.concatenate((test_context, test_questions), axis=-1)\n",
    "\n",
    "    return {\n",
    "        'train_corpus': {\n",
    "            'max_length': (max_train_context_length, max_train_question_length),\n",
    "            'data': concatenated_training_corpus\n",
    "        },\n",
    "        'dev_corpus': {\n",
    "            'max_length': (max_dev_context_length, max_dev_question_length),\n",
    "            'data': concatenated_dev_corpus\n",
    "        },\n",
    "        'test_corpus': {\n",
    "            'max_length': (max_test_context_length, max_test_question_length),\n",
    "            'data': concatenated_test_corpus\n",
    "        },\n",
    "        'embedding': {\n",
    "            'word_index': word_index,\n",
    "            'inverted_word_index': invert_word_index,\n",
    "            'weight_matrix': weight_matrix,\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [10:28<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_data=pre_process(train=train,dev=dev,test=test,documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training corpus context&question length 431 60\n",
      "dev corpus context&question length 156 33\n",
      "test corpus context&question length 155 36\n"
     ]
    }
   ],
   "source": [
    "max_train_context_length, max_train_question_length = processed_data['train_corpus']['max_length']\n",
    "training_corpus = processed_data['train_corpus']['data']\n",
    "max_dev_context_length, max_dev_question_length = processed_data['dev_corpus']['max_length']\n",
    "dev_corpus = processed_data['dev_corpus']['data']\n",
    "max_test_context_length, max_test_question_length = processed_data['test_corpus']['max_length']\n",
    "test_corpus = processed_data['test_corpus']['data']\n",
    "weight_matrix = processed_data['embedding']['weight_matrix']\n",
    "word_index = processed_data['embedding']['word_index']\n",
    "inverted_word_index = processed_data['embedding']['inverted_word_index']\n",
    "\n",
    "train_context_col, train_question_col = (max_train_context_length, max_train_context_length + max_train_question_length)\n",
    "dev_context_col, dev_question_col = (max_dev_context_length, max_dev_context_length + max_dev_question_length)\n",
    "test_context_col, test_question_col = (max_test_context_length, max_test_context_length + max_test_question_length)\n",
    "print('training corpus context&question length', max_train_context_length, max_train_question_length)\n",
    "print('dev corpus context&question length', max_dev_context_length, max_dev_question_length)\n",
    "print('test corpus context&question length', max_test_context_length, max_test_question_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_corpus',training_corpus)\n",
    "np.save('test_corpus',test_corpus)\n",
    "np.save('dev_corpus',dev_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('weight_matrix',weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_index.pkl','wb') as f:\n",
    "    pickle.dump(word_index,f)\n",
    "with open('inverted_word_index.pkl','wb') as f:\n",
    "    pickle.dump(inverted_word_index,f)\n",
    "with open('train_length.pkl','wb') as f:\n",
    "    pickle.dump((max_train_context_length,max_train_question_length),f)\n",
    "with open('test_length.pkl','wb') as f:\n",
    "    pickle.dump((max_test_context_length,max_test_question_length),f)\n",
    "with open('dev_length.pkl','wb') as f:\n",
    "    pickle.dump((max_dev_context_length,max_dev_question_length),f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
